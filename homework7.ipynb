{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zqB_tqMuiXsL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction import _stop_words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "nameOfTag = newsgroups_train.target_names"
      ],
      "metadata": {
        "id": "AseHuA9FjBv7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = 4700\n",
        "n_components = 20\n",
        "n_top_words = 10\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "                    lowercase=True, stop_words=_stop_words.ENGLISH_STOP_WORDS,\n",
        "                    analyzer='word', binary=True,\n",
        "                    max_df=0.75, min_df=2,\n",
        "                    max_features=n_features\n",
        ")\n",
        "X_train = vectorizer.fit_transform(newsgroups_train.data).toarray()"
      ],
      "metadata": {
        "id": "4UELvRLsjQcW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _customLDA(ndk, nkw, nk, _z, _document, _word, _alpha, _beta, _topic,  max_iter=30):\n",
        "    for i in tqdm(range(max_iter)):\n",
        "        for j in range(len(_document)):\n",
        "            cur_word = _word[j]\n",
        "            cur_document = _document[j]\n",
        "            cur_topic = _z[j]\n",
        "            ndk[cur_document, cur_topic] -= 1\n",
        "            nkw[cur_topic, cur_word] -= 1\n",
        "            nk[cur_topic] -= 1\n",
        "            p = (ndk[cur_document, :] + _alpha) * (nkw[:, cur_word] + _beta[cur_word]) / (nk + _beta.sum())\n",
        "            _z[j] = np.random.choice(np.arange(_topic), p = p / p.sum())\n",
        "            ndk[cur_document, _z[j]] += 1\n",
        "            nkw[_z[j], cur_word] += 1\n",
        "            nk[_z[j]] += 1\n",
        "    return ndk, nkw, nk, _z"
      ],
      "metadata": {
        "id": "J1qCyWmplth1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic = 20\n",
        "ndk = np.zeros( topic * X_train.shape[0]).reshape(X_train.shape[0], topic)\n",
        "nkw = np.zeros( topic * X_train.shape[1]).reshape(topic, X_train.shape[1])\n",
        "nk = np.zeros(topic)\n",
        "document, word = X_train.nonzero()\n",
        "z = np.random.choice(topic, len(document))"
      ],
      "metadata": {
        "id": "2Kwx69MzlxGQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, j, k in zip(document, word, z):\n",
        "    ndk[i, k] += 1\n",
        "    nkw[k, j] += 1\n",
        "    nk[k] += 1"
      ],
      "metadata": {
        "id": "RiieTvo_l0NN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndk, nkw,  nk, z = _customLDA(ndk, nkw, nk, z, document, word, np.ones(20), np.ones(X_train.shape[1]), 20, max_iter=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxPos-6mqa8O",
        "outputId": "3dcc7019-268b-4a35-d5e2-ec51416cfe1a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [12:52<00:00, 25.75s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "при max_df =0.95 получал слишком много общеупотребительных слов, уменьшил до 0.75 + увеличил количество итераций"
      ],
      "metadata": {
        "id": "_kOK62ueyrf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = np.argsort(nkw, axis=1)[:, -10:]\n",
        "for i in range(20):\n",
        "    matrix = np.zeros((1, X_train.shape[1]))\n",
        "    for j in result[i]:\n",
        "        matrix[0, j] = 1\n",
        "    print('Tag {} \\t{}'.format(i + 1, '\\t'.join(vectorizer.inverse_transform(matrix)[0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2we_mrHpyIQ",
        "outputId": "cd2576bd-d514-45cf-f9af-b281a4071eee"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag 1 \tearth\tedu\tgordon\tnasa\tresearch\tscience\tsoon\tspace\ttime\tuniversity\n",
            "Tag 2 \tcurrent\tknown\tline\tlist\tnew\torder\tposted\tposting\tread\tway\n",
            "Tag 3 \tbelieve\tbible\tchristian\tdoes\tgod\tjesus\tlife\tpeople\tsay\ttrue\n",
            "Tag 4 \tdon\tisn\tjust\tknow\tlike\tpeople\tthings\tthink\ttime\tve\n",
            "Tag 5 \tago\tdid\tjust\tlike\tpeople\tright\tseen\ttime\tve\tyears\n",
            "Tag 6 \tfile\tfiles\tftp\tprogram\tsoftware\tuse\tusing\tversion\twindow\twindows\n",
            "Tag 7 \tcase\tdon\tgovernment\tgun\tlaw\tmake\tpeople\tright\trights\tstate\n",
            "Tag 8 \taddress\tdiscussion\texample\tgeneral\tgroup\tgroups\thaving\tnumber\tpoint\tquestions\n",
            "Tag 9 \tchip\tclipper\tdata\tencryption\tgovernment\tkey\tkeys\tphone\tpublic\tuse\n",
            "Tag 10 \tcard\tcomputer\tdisk\tdrive\thard\tmac\tmemory\tmonitor\tuse\tvideo\n",
            "Tag 11 \t10\t11\t12\t13\t14\t15\t16\t20\t25\t30\n",
            "Tag 12 \tday\tdid\tgoing\tgot\tleft\tlittle\tright\tstarted\ttook\twent\n",
            "Tag 13 \tcost\tmake\tmoney\tnew\toffer\tpay\tprice\tsale\tsell\tused\n",
            "Tag 14 \tgovernment\thistory\tisrael\tjews\tkilled\tpeople\tstate\twar\tworld\tyears\n",
            "Tag 15 \tactually\tcalled\tcom\tdoes\tgood\tread\ttry\tuse\twant\tway\n",
            "Tag 16 \tdoesn\tdon\tjust\tknow\tlike\tmake\tpeople\treally\tthink\tway\n",
            "Tag 17 \tadvance\tdoes\thelp\thi\tknow\tlike\tlooking\tmail\tpost\tthanks\n",
            "Tag 18 \tgame\tgames\tgood\tleague\tplay\tplayers\tseason\tteam\twin\tyear\n",
            "Tag 19 \tdon\tgoing\tgood\tjust\tknow\tlike\tsaid\tsay\tthing\tthink\n",
            "Tag 20 \tbike\tcar\tcars\tengine\thigh\tjust\tlike\tnew\tpower\troad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "можно выделить несколько тегов которые выделяются в 1 тему:\n",
        "1 космос\n",
        "2 интернет\n",
        "3 вера\n",
        "6 программирование\n",
        "7 обсуждение возможности владеть оружием?\n",
        "И ТД"
      ],
      "metadata": {
        "id": "yHFjDezl2Zsf"
      }
    }
  ]
}